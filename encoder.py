from __future__ import absolute_import
from __future__ import division
from __future__ import print_function

import tensorflow as tf
import numpy as np
import argparse
import os
import json
import glob
import random
import collections
import math
import time

from ops import *

def create_generator_encoder(generator_inputs, a):
    layers = []

    # encoder_1: [batch, 256, 256, in_channels] => [batch, 128, 128, ngf]
    i = 0
    with tf.variable_scope("encoder_1"):
       #output = gen_conv(generator_inputs, a.ngf, a) #CHANGED
       output = res_net(generator_inputs, a.ngf, "encoder")
       #print(i)
       #print(output.shape)
       i += 1
       layers.append(output)

    layer_specs = [
        a.ngf * 2, # encoder_2: [batch, 128, 128, ngf] => [batch, 64, 64, ngf * 2]
        a.ngf * 4, # encoder_3: [batch, 64, 64, ngf * 2] => [batch, 32, 32, ngf * 4]
        a.ngf * 8, # encoder_4: [batch, 32, 32, ngf * 4] => [batch, 16, 16, ngf * 8]
        a.ngf * 8, # encoder_5: [batch, 16, 16, ngf * 8] => [batch, 8, 8, ngf * 8]
        # Latent representation has 8x8 dimensionality
    ]

    for out_channels in layer_specs:
        with tf.variable_scope("encoder_%d" % (len(layers) + 1)):
            rectified = lrelu(layers[-1], 0.2)
            # [batch, in_height, in_width, in_channels] => [batch, in_height/2, in_width/2, out_channels]
            #convolved = gen_conv(rectified, out_channels, a) #CHANGED
            convolved = res_net(rectified, out_channels, "encoder")
            #print(i)
            #print(convolved.shape)
            i += 1
            output = batchnorm(convolved)
            layers.append(output)

    # Exclusive part of representation uses FC layer
    with tf.variable_scope("encoder_exclusive"):
        rinput = tf.reshape(rectified, [-1, 16*16*8*a.ngf])
        outputE = gen_fc(rinput,out_channels=8) #CHANGED


    sR = output
    eR = outputE


    return sR, eR

